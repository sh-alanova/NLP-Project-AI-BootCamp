import torch
from tqdm import tqdm

class LIMTracker:
    def __init__(self, model, target_layer_types=(torch.nn.Linear,)):
        self.model = model
        self.lim_data = {}  # {layer_name: {'input': [], 'output': []}}
        self.hooks = []
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ö—É–∫–∏ –Ω–∞ –≤—Å–µ —Ü–µ–ª–µ–≤—ã–µ —Å–ª–æ–∏
        for name, layer in model.named_modules():
            if isinstance(layer, target_layer_types):
                self.lim_data[name] = {'input': None, 'output': None}
                self.hooks.append(layer.register_forward_hook(self._create_hook(name)))

    def _create_hook(self, layer_name):
        def hook(module, input, output):
            self.lim_data[layer_name]['input'] = input[0].detach().clone()
            self.lim_data[layer_name]['output'] = output.detach().clone()
        return hook

    def compute_lim(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {layer_name: lim_value}"""
        lim_results = {}
        for name, data in self.lim_data.items():
            if data['input'] is not None and data['output'] is not None:
                lim = torch.norm(data['output'] - data['input']) / torch.norm(data['input'])
                lim_results[name] = lim.item()
        return lim_results

    def remove_hooks(self):
        for hook in self.hooks:
            hook.remove()



### Main

def main():
    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã—Ö (–≤–∞—à –∫–æ–¥)
    model = ...  # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å Qwen-2.5
    calib_data = ...  # –í–∞—à–∏ –∫–∞–ª–∏–±—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

    # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–∫–µ—Ä–∞ LIM
    # –£–∫–∞–∑—ã–≤–∞–µ–º —Ç–∏–ø—ã —Å–ª–æ—ë–≤, –∫–æ—Ç–æ—Ä—ã–µ —Ö–æ—Ç–∏–º –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å (–Ω–∞–ø—Ä., LinearQuantizer)
    lim_tracker = LIMTracker(model, target_layer_types=(LinearQuantizer,))

    # 3. –ü—Ä–æ–≥–æ–Ω –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
    with torch.no_grad():
        for batch in tqdm(calib_data, desc="–í—ã—á–∏—Å–ª–µ–Ω–∏–µ LIM"):
            if isinstance(batch, (tuple, list)):
                model(*batch)
            else:
                model(batch)

    # 4. –ü–æ–ª—É—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    lim_results = lim_tracker.compute_lim()
    
    print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã LIM:")
    for name, lim in lim_results.items():
        print(f"{name}: {lim:.4f}")

    # 5. –û—á–∏—Å—Ç–∫–∞ —Ö—É–∫–æ–≤ (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!)
    lim_tracker.remove_hooks()

    # (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
    import json
    with open("lim_results.json", "w") as f:
        json.dump(lim_results, f, indent=2)

  ```
üîπ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
 - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å–ª–æ—ë–≤: –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Å–ª–æ–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º –∞–∫—Ç–∏–≤–∞—Ü–∏–π):

  target_layers = [name for name, layer in model.named_modules() 
                  if isinstance(layer, LinearQuantizer) and layer.quant_input.is_enable]
  lim_tracker = LIMTracker(model, target_layer_names=target_layers)


 - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è

  import matplotlib.pyplot as plt
  
  def plot_lim(lim_results):
      names = list(lim_results.keys())
      values = list(lim_results.values())
      
      plt.figure(figsize=(12, 6))
      plt.bar(range(len(values)), values, tick_label=names)
      plt.xticks(rotation=90)
      plt.ylabel("LIM")
      plt.title("–ò—Å–∫–∞–∂–µ–Ω–∏–µ –ø—Ä–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–∏ –ø–æ —Å–ª–æ—è–º")
      plt.tight_layout()
      plt.show()
  
  plot_lim(lim_results)


üîπ –í–∞–∂–Ω—ã–µ –∑–∞–º–µ—á–∞–Ω–∏—è
 - –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: –•—É–∫–∏ –¥–æ–±–∞–≤–ª—è—é—Ç –æ–≤–µ—Ä—Ö–µ–¥ (~5-10% slowdown).
 - –ü–∞–º—è—Ç—å: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤ –º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ .cpu()).
 - –î–ª—è Qwen-2.5: –û—Å–æ–±–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ LinearQuantizer –∏ Conv2dQuantizer.
```

–ü—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ **LIM (Layer Input Modification)** –¥–ª—è —Å–ª–æ—ë–≤, –≥–¥–µ –≤—Ö–æ–¥ –∏ –≤—ã—Ö–æ–¥ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, `Linear` —Å–ª–æ–π: –≤—Ö–æ–¥ `[batch, seq_len, dim_in]`, –≤—ã—Ö–æ–¥ `[batch, seq_len, dim_out]`), –Ω—É–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫—É. –í–æ—Ç —Ä–µ—à–µ–Ω–∏—è:

---

### üîπ **1. –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–∞–∫–∏–µ —Å–ª–æ–∏ (–ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–±)**
–ï—Å–ª–∏ —Ä–∞–∑–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–µ–ª–∞–µ—Ç LIM –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω–æ–π, –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–∏ —Å–ª–æ–∏:
```python
def compute_lim(self):
    lim_results = {}
    for name, data in self.lim_data.items():
        if data['input'] is None or data['output'] is None:
            continue
        if data['input'].shape != data['output'].shape:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–æ–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è–º–∏
            continue
        lim = torch.norm(data['output'] - data['input']) / torch.norm(data['input'])
        lim_results[name] = lim.item()
    return lim_results
```

---

### üîπ **2. –°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–±—â–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (–¥–ª—è Linear/Conv)**
–î–ª—è —Å–ª–æ—ë–≤, –≥–¥–µ –º–µ–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, `dim_in` ‚Üí `dim_out`), –º–æ–∂–Ω–æ:
- –£—Å–µ—á—å –≤—Ö–æ–¥ –¥–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—ã—Ö–æ–¥–∞.
- –í—ã—á–∏—Å–ª–∏—Ç—å LIM —Ç–æ–ª—å–∫–æ –ø–æ –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–º—Å—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è–º.

#### –ü—Ä–∏–º–µ—Ä –¥–ª—è `Linear` —Å–ª–æ—è:
```python
if input.shape[:-1] != output.shape[:-1]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π
    continue

# –£—Å–µ–∫–∞–µ–º –≤—Ö–æ–¥ –¥–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –≤—ã—Ö–æ–¥–∞
input_truncated = input[..., :output.shape[-1]]
lim = torch.norm(output - input_truncated) / torch.norm(input_truncated)
```

---

### üîπ **3. –û—Ç–¥–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è Attention-—Å–ª–æ—ë–≤**
–í Transformer-—Å–ª–æ—è—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, `q_proj`, `k_proj`) –≤—ã—Ö–æ–¥ –º–æ–∂–µ—Ç –∏–º–µ—Ç—å –¥—Ä—É–≥—É—é —Ñ–æ—Ä–º—É. –ó–¥–µ—Å—å LIM –º–æ–∂–Ω–æ —Å—á–∏—Ç–∞—Ç—å –¥–ª—è **–ø—Ä–æ–µ–∫—Ü–∏–π –≤–Ω—É—Ç—Ä–∏ Attention**:
```python
# –î–ª—è Q/K/V –ø—Ä–æ–µ–∫—Ü–∏–π –≤ Attention
if 'self_attn.q_proj' in name:
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã –¥–æ/–ø–æ—Å–ª–µ –ø—Ä–æ–µ–∫—Ü–∏–∏ –ø–æ —Å–∫—Ä—ã—Ç–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    lim = torch.norm(output - input[..., :output.shape[-1]]) / torch.norm(input[..., :output.shape[-1]])
```

---

### üîπ **4. –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)**
–ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –º–µ—Ç–æ–¥—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Å–ª–æ—è:
```python
def compute_lim(self):
    lim_results = {}
    for name, data in self.lim_data.items():
        if data['input'] is None or data['output'] is None:
            continue

        input_tensor = data['input']
        output_tensor = data['output']

        # –°–ª—É—á–∞–π 1: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, LayerNorm)
        if input_tensor.shape == output_tensor.shape:
            lim = torch.norm(output_tensor - input_tensor) / torch.norm(input_tensor)

        # –°–ª—É—á–∞–π 2: Linear/Conv —Å –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        elif (input_tensor.dim() == output_tensor.dim() and 
              input_tensor.shape[:-1] == output_tensor.shape[:-1]):
            input_truncated = input_tensor[..., :output_tensor.shape[-1]]
            lim = torch.norm(output_tensor - input_truncated) / torch.norm(input_truncated)

        # –°–ª—É—á–∞–π 3: Attention-–ø—Ä–æ–µ–∫—Ü–∏–∏ (Q/K/V)
        elif 'self_attn' in name and ('q_proj' in name or 'k_proj' in name or 'v_proj' in name):
            lim = self._compute_attention_lim(input_tensor, output_tensor)

        else:
            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏

        lim_results[name] = lim.item()
    return lim_results

def _compute_attention_lim(self, input, output):
    """–°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è Attention-—Å–ª–æ—ë–≤"""
    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –¥–ª—è Multi-Head Attention
    return torch.norm(output - input.mean(dim=-1, keepdim=True)) / torch.norm(input)
```

---

### üîπ **5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–ª–æ—ë–≤**
–î–æ–±–∞–≤—å—Ç–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏:
```python
for name, data in self.lim_data.items():
    if data['input'] is not None and data['output'] is not None:
        if data['input'].shape != data['output'].shape:
            print(f"–°–ª–æ–π {name}: —Ä–∞–∑–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å. –í—Ö–æ–¥: {data['input'].shape}, –í—ã—Ö–æ–¥: {data['output'].shape}")
```

---

### üîπ **–ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞**
```
–°–ª–æ–π model.layers.0.self_attn.q_proj: —Ä–∞–∑–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å. –í—Ö–æ–¥: [1, 256, 4096], –í—ã—Ö–æ–¥: [1, 256, 512]
–°–ª–æ–π model.layers.0.mlp.down_proj: —Ä–∞–∑–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å. –í—Ö–æ–¥: [1, 256, 512], –í—ã—Ö–æ–¥: [1, 256, 4096]
```

---

### üîπ **–ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏**
1. **–î–ª—è Linear/Conv-—Å–ª–æ—ë–≤** –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —É—Å–µ—á–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞ (–ø—É–Ω–∫—Ç 2).  
2. **–î–ª—è Attention-—Å–ª–æ—ë–≤** —Ä–µ–∞–ª–∏–∑—É–π—Ç–µ –æ—Ç–¥–µ–ª—å–Ω—É—é –ª–æ–≥–∏–∫—É (–ø—É–Ω–∫—Ç 3).  
3. **–°–ª–æ–∏ —Å –ø–æ–ª–Ω—ã–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã** (–Ω–∞–ø—Ä–∏–º–µ—Ä, `Reshape`) –ª—É—á—à–µ –∏—Å–∫–ª—é—á–∏—Ç—å.  
4. **–õ–æ–≥–∏—Ä—É–π—Ç–µ** –≤—Å–µ —Å–ª—É—á–∞–∏ –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.  
